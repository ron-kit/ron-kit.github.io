<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ron Kitainik: Blog &amp; Portfolio/projects/enph353/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="https://ron-kit.github.io/hugo-theme-console/css/terminal-0.7.4.min.css">
    <link rel="stylesheet" href="https://ron-kit.github.io/hugo-theme-console/css/animate-4.1.1.min.css">
    <link rel="stylesheet" href="https://ron-kit.github.io/hugo-theme-console/css/console.css">
    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
    <link rel="icon" href="img/favicon.ico" type="image/x-icon">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<meta property="og:title" content="Simulated Self-Driving Robot" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ron-kit.github.io/projects/enph353/" />


  
<meta property="og:image" content="https://ron-kit.github.io/projects/enph353/edge%20detection_hu_da4e94fcb1dcba0b.png">

<meta property="article:published_time" content="2025-01-17T00:00:00+00:00" />

<link rel="stylesheet" href="/css/custom.css">



<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]      
    },
    loader:{
      load: ['ui/safe']
    },
  };
</script>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              <a href="https://ron-kit.github.io/" class="no-style site-name">Ron Kitainik: Blog &amp; Portfolio</a>:~# 
              <a href='https://ron-kit.github.io/projects'>projects</a>/<a href='https://ron-kit.github.io/projects/enph353'>enph353</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="https://ron-kit.github.io/about/" typeof="ListItem">about/</a></li>
                
                <li><a href="https://ron-kit.github.io/posts/" typeof="ListItem">posts/</a></li>
                
                <li><a href="https://ron-kit.github.io/projects/" typeof="ListItem">projects/</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container " >
        
<h1>Simulated Self-Driving Robot</h1>


    
    
        
        <img src="/projects/enph353/edge%20detection_hu_b23b08fb96e34506.png" alt="edge detection.png" class="img-responsive gallery-image">
    


<p>A virtual robot that detects features and navigates terrain to compete in a race. Programmed with OpenCV, connected with ROS, and simulated in Gazebo.</p>
<hr>
<h2 id="competition-map">Competition Map</h2>
<div style="text-align: center">
    <img src="353map.gif" width="100%" height="auto">
    <p><i></i></p>
</div>
<h2 id="development-log">Development Log</h2>
<p>I started with a very basic prebuilt robot, not much more than a box on wheels, and developed a system to detect the road.
The standard approach to this is to apply a Gaussian blur followed by a binary threshold, but I opted for HSV thresholding instead (seen in the cover image), which procured a very strong boundary between road and ground.</p>
<video width="100%" height="auto" controls>
    <source src="tracked_video_feed.mov" type="video/mp4">
    Your browser does not support the video tag.
</video>

<p>Using a pregenerated video of the robot driving a lap, I added realtime frame processing to locate the the road.
By taking the centroid of the road segment detected in the bottom quarter of each frame, my algorithm was able to very accurately place a circle on the middle of the road.</p>
<p>This would be useful for navigation later, but for now I pivoted to another task: keypoint detection with SIFT.</p>
<hr>
<div style="text-align: center">
    <img src="keypoints.png" width="100%" height="auto">
    <p><i></i></p>
</div>
<p>I made a basic UI in Qt to import reference images and track my camera feed. <br>
Using SIFT I located and displayed the keypoints (small regions with a transformation-invariant color gradient) on the live feed.
Invariance is a pretty abstract classification but it seems to pick out centers (as seen on the lights) and edges (as seen on me) most often.</p>
<div style="text-align: center">
    <img src="snickers-recognized.png" width="100%" height="auto">
    <p><i></i></p>
</div>
<p>A point mapping from reference to frame is dynamically generated using Flann (method of locating closest vector pairs in gradient space)
and RANSAC (a probabilistic sampling technique). Finally a homography is drawn to highlight the detected image. <br>
Here it is recognizing a photo of my cat, even under a slight skew.</p>
<hr>
<p>To lay the foundation for CNNs I followed alongside Karpathy&rsquo;s tutorial on gradient descent
and used it to make a simple linear regression generator with a squared difference loss function.</p>
<p>The application of this project was to text detection. Building off TensorFlow features I developed and trained a deep learning MLP network.
With only 20 epochs of training it reached about 90% accuracy in classifying letters and numbers.</p>
<p>[TODO: CONFUSION MATRIX HERE]</p>
<hr>
<p>Having implemented these sub-behaviours I approached the real challenge: training the robot to navigate through the environment.</p>
<p>There are many variables at play here: the road changes height, material, and color, and obstacles, both rigid and moving, hinder the robot&rsquo;s movement.</p>
<p>I made a basic Q-learning system to start, using the HSV thresholding from before.
The learning is implemented with the Bellman equation, which uses the following parameters:</p>
<ul>
<li>Alpha: learning rate.</li>
<li>Gamma: value of observation relative to known rewards.</li>
<li>Epsilon: exploration factor.</li>
</ul>
<div style="text-align: center">
    <img src="robot_improving.png" width="100%" height="auto">
    <p><i></i></p>
</div>
<p>With a simple, static reward function, as well as a pretty high epsilon, the robot is nevertheless able to learn and improve. The rewards plot is a bit erratic as the robot randomly veers off the track in tireless pursuit of escaping local minima, but exhibits a clear upward trend.</p>




        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
